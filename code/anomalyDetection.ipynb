{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pyod.models.lof import LOF\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247010, 121)\n",
      "(208612, 121) (208612,) (247011, 121) (247011,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# data generation for kdd99\n",
    "\n",
    "url_base = \"http://kdd.ics.uci.edu/databases/kddcup99\"\n",
    "\n",
    "# KDDCup 10% Data\n",
    "url_data = f\"{url_base}/kddcup.data_10_percent.gz\"\n",
    "# info data (column names, col types)\n",
    "url_info = f\"{url_base}/kddcup.names\"\n",
    "\n",
    "# Import info data\n",
    "df_info = pd.read_csv(url_info, sep=\":\", skiprows=1, index_col=False, names=[\"colname\", \"type\"])\n",
    "colnames = df_info.colname.values\n",
    "coltypes = np.where(df_info[\"type\"].str.contains(\"continuous\"), \"float\", \"str\")\n",
    "colnames = np.append(colnames, [\"status\"])\n",
    "coltypes = np.append(coltypes, [\"str\"])\n",
    "\n",
    "# Import data\n",
    "df = pd.read_csv(url_data, names=colnames, index_col=False,\n",
    "                 dtype=dict(zip(colnames, coltypes)))\n",
    "# df = pd.read_csv('../data/kddcup.data_10_percent_corrected', names=colnames, index_col=False,\n",
    "#                  dtype=dict(zip(colnames, coltypes)))\n",
    "# Dumminize\n",
    "X = pd.get_dummies(df.iloc[:,:-1]).values\n",
    "\n",
    "# Normalize\n",
    "scaler = scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Create Traget Flag\n",
    "# Anomaly data when status is normal, Otherwise, Not anomaly.\n",
    "y = np.where(df.status == \"normal.\", -1, 0)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=123)\n",
    "# X_train, y_train = X_train[y_train == 0], y_train[y_train == 0]\n",
    "# X_test, y_test = X_test[y_test == 0], y_test[y_test == 0]\n",
    "\n",
    "# X_anomaly, y_anomaly = X_train[y_train == -1][-10000:], y_train[y_train == -1][-10000:]\n",
    "X_anomaly, y_anomaly = X_train[y_train == -1][:10000], y_train[y_train == -1][:10000]\n",
    "X_normal, y_normal = X_train[y_train == 0], y_train[y_train == 0]\n",
    "print(X_train.shape)\n",
    "X_train = np.concatenate((X_normal,X_anomaly),axis=0)\n",
    "y_train = np.concatenate((y_normal,y_anomaly),axis=0)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# Save traning and testing data\n",
    "# np.save('../data/kddcup99/X_train.npy',X_train)\n",
    "# np.save('../data/kddcup99/X_test.npy', X_test)\n",
    "# np.save('../data/kddcup99/y_train.npy', y_train)\n",
    "# np.save('../data/kddcup99/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11013, 36) (11013,)\n",
      "(4950, 36) (4950,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nana/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "# Import data\n",
    "\n",
    "df = pd.read_csv('../data/letter/letter.csv')\n",
    "# print(df)\n",
    "X_anomaly = df[df['Class'] == 5]\n",
    "X_normal = df[df['Class'] == 1]\n",
    "\n",
    "X_anomaly = X_anomaly.replace(['?','$'],method='bfill')\n",
    "X_anomaly = X_anomaly.replace([0,'$'],method='ffill')\n",
    "X_anomaly = X_anomaly.replace('?',0)\n",
    "\n",
    "X_normal = X_normal.replace(['?','$'],method='bfill')\n",
    "X_normal = X_normal.replace([0,'$'],method='ffill')\n",
    "X_normal = X_normal.replace('?',0)\n",
    "\n",
    "\n",
    "\n",
    "X_anomaly = np.array(X_anomaly.iloc[:15000, 2:])\n",
    "X_normal = np.array(X_normal.iloc[:1500, 2:])\n",
    "\n",
    "X = np.concatenate((X_anomaly, X_normal),axis=0)\n",
    "y = np.concatenate(([0 for i in range(len(X_anomaly))], [-1 for i in range(len(X_normal))]),axis=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "# X_test, y_test = X_test[y_test == -1], y_test[y_test == -1]\n",
    "\n",
    "X_temp, y_temp = X_train[y_train == -1], y_train[y_train == -1]\n",
    "X_train, y_train = X_train[y_train == 0], y_train[y_train == 0]\n",
    "\n",
    "X_train =  np.concatenate((X_train, X_temp[:len(X_temp) // 2]),axis=0)\n",
    "y_train =  np.concatenate((y_train, y_temp[:len(y_temp) // 2]),axis=0)\n",
    "# X_test = np.concatenate((X_test, X_temp[len(X_temp) // 2:]),axis=0)\n",
    "# y_test = np.concatenate((y_test, y_temp[len(X_temp) // 2:]),axis=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "# print(len(np.where(y_test == -1)[0]))\n",
    "# Normalize\n",
    "scaler = scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n",
    "# np.save('../data/letter/X_train.npy',X_train)\n",
    "# np.save('../data/letter/X_test.npy', X_test)\n",
    "# np.save('../data/letter/y_train.npy', y_train)\n",
    "# np.save('../data/letter/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19016, 13)\n",
      "(12749, 13) (1000, 13) (5705, 13)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "# data generation for thyroid\n",
    "\n",
    "# Import data\n",
    "\n",
    "filenames = os.listdir('../data/thyroid/')\n",
    "\n",
    "data = []\n",
    "for filename in filenames:\n",
    "    if filename.strip().split('.')[-1] == 'data' or filename.split('.')[-1] == 'test':\n",
    "        with open('../data/thyroid/' + filename,'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                record = line.split('|')[0].split('[')[0].split(',')\n",
    "                if record.count('?') > 1:\n",
    "                    continue\n",
    "                data.append(record)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# padding missing data\n",
    "df = df.replace(['?','$'],method='pad')\n",
    "df = df.replace('?',0)\n",
    "df = df.iloc[:,[0,1,17,19,21,23,25,27,28,-1]]\n",
    "# print(df)\n",
    "\n",
    "# Dumminize\n",
    "df = df.replace(['F','M'], [0,1])\n",
    "X = pd.concat([df.iloc[:,:-1],pd.get_dummies(df.iloc[:,-2:-1])],axis=1)\n",
    "X.drop([28],axis=1, inplace=True)\n",
    "X = np.array(X).astype(np.float)\n",
    "print(X.shape)\n",
    "\n",
    "# Normalize\n",
    "scaler = scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# X = min_max_scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# Create Traget Flag\n",
    "# Anomaly data when status is normal, Otherwise, Not anomaly.\n",
    "y = np.where((df.iloc[:,-1] == \"negative.\") | (df.iloc[:,-1] == \"-\"), 0, -1)\n",
    "\n",
    "# Split Data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)\n",
    "# X_test, y_test = X_train[y_train == 0], y_train[y_train == 0]\n",
    "# X_test, y_test = X_test[y_test == -1], y_test[y_test == -1]\n",
    "\n",
    "      \n",
    "X_temp = X_train[y_train == -1][:1000]\n",
    "y_temp = y_train[y_train == -1][:1000]\n",
    "X_train, y_train = X_train[y_train == 0], y_train[y_train == 0]\n",
    "X_train = np.concatenate((X_train,X_temp),axis=0)\n",
    "y_train = np.concatenate((y_train,y_temp),axis=0)\n",
    "print(X_train.shape, X_temp.shape, X_test.shape)\n",
    "# print(len(np.where(y_test == -1)[0]))\n",
    "# Save traning and testing data\n",
    "# np.save('../data/thyroid/X_train.npy',X_train)\n",
    "# np.save('../data/thyroid/X_test.npy', X_test)\n",
    "# np.save('../data/thyroid/y_train.npy', y_train)\n",
    "# np.save('../data/thyroid/y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '../data/thyroid/'\n",
    "\n",
    "# anomaly\n",
    "x2 = np.load(datadir + 'std1.npy').squeeze()\n",
    "y2 = np.load(datadir + 'mu_loss1.npy').squeeze()\n",
    "# normal\n",
    "x1 = np.load(datadir + 'std.npy').squeeze()\n",
    "y1 = np.load(datadir + 'mu_loss.npy').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2sum = np.sum(x2, axis = 1).squeeze()\n",
    "y2sum = np.sum(y2, axis = 1).squeeze()\n",
    "x1sum = np.sum(x1, axis = 1).squeeze()\n",
    "y1sum = np.sum(y1, axis = 1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 13) (5099, 13) (606, 5) (5099, 5)\n",
      "288\n",
      "288\n",
      "anomaly: (288, 18)\n",
      "normal: (2823, 18)\n",
      "total instance: (3111, 18)\n",
      "total label: (3111,)\n",
      "(5705, 18)\n"
     ]
    }
   ],
   "source": [
    "# first is normal, secord is anomaly\n",
    "\n",
    "# kdd99 \n",
    "upper = 102.5 #10000000 #102.5 #58.5  #46    #20.22\n",
    "lower = 0.1\n",
    "# upper1 = 20.15  #7.6      #16    #20.06 #20.15 #nan \n",
    "\n",
    "# letter\n",
    "upper = 41.6  #31.3 #33.8 #37 #41.6\n",
    "lower = 17.5\n",
    "\n",
    "# thyroid\n",
    "upper = 0.95 #0.4  #0.57 #0.95 #2\n",
    "lower = 0.04\n",
    "\n",
    "# y1 = y1sum\n",
    "# y2 = y2sum\n",
    "# x1 = x1sum\n",
    "# x2 = x2sum\n",
    "print(y1.shape, y2.shape, x1.shape, x2.shape)\n",
    "\n",
    "# anomalyInstance = np.array([y1[np.where(y1sum > upper)[0]], x1[np.where(y1sum > upper)[0]]]).T\n",
    "anomalyInstance = np.concatenate((y1[np.where(y1sum > upper)[0]], x1[np.where(y1sum > upper)[0]]), axis=1)\n",
    "# tmp = np.array([y2[np.where(y2sum > upper)[0]], x2[np.where(y2sum > upper)[0]]]).T\n",
    "tmp = np.concatenate((y2[np.where(y2sum > upper)[0]], x2[np.where(y2sum > upper)[0]]), axis=1)\n",
    "# print(len(anomalyInstance) + len(tmp))\n",
    "# anomalyInstance = np.array([y1[np.where(y1 > upper1)[0]], x1[np.where(y1 > upper1)[0]]]).T\n",
    "# anomalyInstance = np.concatenate((y1[np.where(y1sum > upper1)[0]], x1[np.where(y1sum > upper1)[0]]), axis=1)\n",
    "# print(len(anomalyInstance) + len(tmp))\n",
    "\n",
    "\n",
    "anomalyInstance = np.concatenate((anomalyInstance, tmp), axis=0)\n",
    "print('anomaly:',anomalyInstance.shape)\n",
    "\n",
    "\n",
    "normalInstance = []\n",
    "# normalInstance = np.array([y1[np.where(y1 < lower)[0]], x1[np.where(y1 < lower)[0]]]).T\n",
    "normalInstance = np.concatenate((y1[np.where(y1sum < lower)[0]], x1[np.where(y1sum < lower)[0]]), axis=1)\n",
    "# tmp = np.array([y2[np.where(y2 < lower)[0]], x2[np.where(y2 < lower)[0]]]).T\n",
    "tmp = np.concatenate((y2[np.where(y2sum < lower)[0]], x2[np.where(y2sum < lower)[0]]), axis=1)\n",
    "\n",
    "normalInstance = np.concatenate((normalInstance, tmp), axis=0)\n",
    "print('normal:', normalInstance.shape)\n",
    "instances = np.concatenate((anomalyInstance, normalInstance), axis=0)\n",
    "print('total instance:', instances.shape)\n",
    "labels = [0 for i in range(len(anomalyInstance))]\n",
    "labels.extend([1 for i in range(len(normalInstance))])\n",
    "labels = np.array(labels)\n",
    "print('total label:', labels.shape)\n",
    "for i in range(1, len(normalInstance) // len(anomalyInstance)):\n",
    "    instances = np.concatenate((instances, anomalyInstance), axis=0)\n",
    "    labels = np.concatenate((labels, [0 for j in range(len(anomalyInstance))]), axis=0)\n",
    "#     print(instances.shape, labels.shape)\n",
    "\n",
    "np.save('../data/activity/training_data.npy', instances)\n",
    "np.save('../data/activity/traning_label.npy', labels)\n",
    "# print(instances)\n",
    "# print(labels)\n",
    "# anomalyInstance = np.array([y1, x1]).T\n",
    "anomalyInstance = np.concatenate((y1, x1), axis=1)\n",
    "# normalInstance = np.array([y2, x2]).T\n",
    "normalInstance = np.concatenate((y2, x2), axis=1)\n",
    "instances = np.concatenate((anomalyInstance, normalInstance), axis=0)\n",
    "\n",
    "labels = [0 for i in range(len(anomalyInstance))]\n",
    "labels.extend([1 for i in range(len(normalInstance))])\n",
    "labels = np.array(labels)\n",
    "# print(instances.shape)\n",
    "# print(instances)\n",
    "# print(labels)\n",
    "np.save('../data/activity/testing_data.npy', instances) \n",
    "np.save('../data/activity/testing_label.npy', labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
